% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/read_scan.R
\name{from_csv}
\alias{from_csv}
\alias{read_csv_polars}
\alias{scan_csv_polars}
\title{Import data from CSV file(s)}
\usage{
read_csv_polars(
  source,
  ...,
  has_header = TRUE,
  separator = ",",
  comment_prefix = NULL,
  quote_char = "\\"",
  skip_rows = 0,
  schema = NULL,
  schema_overrides = NULL,
  null_values = NULL,
  ignore_errors = FALSE,
  cache = FALSE,
  infer_schema_length = 100,
  n_rows = NULL,
  encoding = "utf8",
  low_memory = FALSE,
  rechunk = TRUE,
  skip_rows_after_header = 0,
  row_index_name = NULL,
  row_index_offset = 0,
  try_parse_dates = FALSE,
  eol_char = "\\n",
  raise_if_empty = TRUE,
  truncate_ragged_lines = FALSE,
  include_file_paths = NULL,
  dtypes,
  reuse_downloaded
)

scan_csv_polars(
  source,
  ...,
  has_header = TRUE,
  separator = ",",
  comment_prefix = NULL,
  quote_char = "\\"",
  skip_rows = 0,
  schema = NULL,
  schema_overrides = NULL,
  null_values = NULL,
  ignore_errors = FALSE,
  cache = FALSE,
  infer_schema_length = 100,
  n_rows = NULL,
  encoding = "utf8",
  low_memory = FALSE,
  rechunk = TRUE,
  skip_rows_after_header = 0,
  row_index_name = NULL,
  row_index_offset = 0,
  try_parse_dates = FALSE,
  eol_char = "\\n",
  raise_if_empty = TRUE,
  truncate_ragged_lines = FALSE,
  include_file_paths = NULL,
  dtypes,
  reuse_downloaded
)
}
\arguments{
\item{source}{Path(s) to a file or directory. When needing to authenticate
for scanning cloud locations, see the \code{storage_options} parameter.}

\item{...}{These dots are for future extensions and must be empty.}

\item{has_header}{Indicate if the first row of dataset is a header or not.If
\code{FALSE}, column names will be autogenerated in the following format:
\code{"column_x"} with \code{x} being an enumeration over every column in the dataset
starting at 1.}

\item{separator}{Single byte character to use as separator in the file.}

\item{comment_prefix}{A string, which can be up to 5 symbols in length, used
to indicate the start of a comment line. For instance, it can be set to \verb{#}
or \verb{//}.}

\item{quote_char}{Single byte character used for quoting. Set to \code{NULL} to
turn off special handling and escaping of quotes.}

\item{skip_rows}{Start reading after a particular number of rows. The header
will be parsed at this offset.}

\item{schema}{Provide the schema. This means that polars doesn't do schema
inference. This argument expects the complete schema, whereas
\code{schema_overrides} can be used to partially overwrite a schema. This must be
a list. Names of list elements are used to match to inferred columns.}

\item{schema_overrides}{Overwrite dtypes during inference. This must be a
list. Names of list elements are used to match to inferred columns.}

\item{null_values}{Character vector specifying the values to interpret as
\code{NA} values. It can be named, in which case names specify the columns in
which this replacement must be made (e.g. \code{c(col1 = "a")}).}

\item{ignore_errors}{Keep reading the file even if some lines yield errors.
You can also use \code{infer_schema = FALSE} to read all columns as UTF8 to
check which values might cause an issue.}

\item{cache}{Cache the result after reading.}

\item{infer_schema_length}{The maximum number of rows to scan for schema
inference. If \code{NULL}, the full data may be scanned (this is slow). Set
\code{infer_schema = FALSE} to read all columns as \code{pl$String}.}

\item{n_rows}{Stop reading from the source after reading \code{n_rows}.}

\item{encoding}{Either \code{"utf8"} or \code{"utf8-lossy"}. Lossy means that invalid
UTF8 values are replaced with "?" characters.}

\item{low_memory}{Reduce memory pressure at the expense of performance.}

\item{rechunk}{Reallocate to contiguous memory when all chunks/files are parsed.}

\item{skip_rows_after_header}{Skip this number of rows when the header is
parsed.}

\item{row_index_name}{If not \code{NULL}, this will insert a row index column with
the given name.}

\item{row_index_offset}{Offset to start the row index column (only used if
the name is set by \code{row_index_name}).}

\item{try_parse_dates}{Try to automatically parse dates. Most ISO8601-like
formats can be inferred, as well as a handful of others. If this does not
succeed, the column remains of data type \code{pl$String}.}

\item{eol_char}{Single byte end of line character (default: \code{"\n"}). When
encountering a file with Windows line endings (\code{"\r\n"}), one can go with the
default \code{"\n"}. The extra \code{"\r"} will be removed when processed.}

\item{raise_if_empty}{If \code{FALSE}, parsing an empty file returns an empty
DataFrame or LazyFrame.}

\item{truncate_ragged_lines}{Truncate lines that are longer than the schema.}

\item{include_file_paths}{Include the path of the source file(s) as a column
with this name.}

\item{dtypes}{\ifelse{html}{\href{https://lifecycle.r-lib.org/articles/stages.html#deprecated}{\figure{lifecycle-deprecated.svg}{options: alt='[Deprecated]'}}}{\strong{[Deprecated]}} Deprecated,
use \code{schema_overrides} instead.}

\item{reuse_downloaded}{\ifelse{html}{\href{https://lifecycle.r-lib.org/articles/stages.html#deprecated}{\figure{lifecycle-deprecated.svg}{options: alt='[Deprecated]'}}}{\strong{[Deprecated]}}
Deprecated with no replacement.}
}
\value{
The scan function returns a LazyFrame, the read function returns a DataFrame.
}
\description{
\code{read_csv_polars()} imports the data as a Polars DataFrame.

\code{scan_csv_polars()} imports the data as a Polars LazyFrame.
}
\examples{
\dontshow{if (require("dplyr", quietly = TRUE) && require("withr", quietly = TRUE)) (if (getRversion() >= "3.4") withAutoprint else force)(\{ # examplesIf}
### Read or scan a single CSV file ------------------------

# Setup: create a CSV file
dest <- withr::local_tempfile(fileext = ".csv")
write.csv(mtcars, dest, row.names = FALSE)

# Import this file as a DataFrame for eager evaluation
read_csv_polars(dest) |>
  arrange(mpg)

# Import this file as a LazyFrame for lazy evaluation
scan_csv_polars(dest) |>
  arrange(mpg) |>
  compute()


### Change the datatype of some columns when reading the file ------------------------

scan_csv_polars(
  dest,
  schema_overrides = list(gear = polars::pl$String, carb = polars::pl$Float32)
) |>
  arrange(mpg) |>
  compute()


### Read or scan several all CSV files in a folder ------------------------

# Setup: create a folder "output" that contains two CSV files
dest_folder <- withr::local_tempdir(tmpdir = "output")
dir.create(dest_folder, showWarnings = FALSE)
dest1 <- file.path(dest_folder, "output_1.csv")
dest2 <- file.path(dest_folder, "output_2.csv")

write.csv(mtcars[1:16, ], dest1, row.names = FALSE)
write.csv(mtcars[17:32, ], dest2, row.names = FALSE)
list.files(dest_folder)

# Import all files as a LazyFrame
scan_csv_polars(dest_folder) |>
  arrange(mpg) |>
  compute()

# Include the file path to know where each row comes from
scan_csv_polars(dest_folder, include_file_paths = "file_path") |>
  arrange(mpg) |>
  compute()


### Read or scan all CSV files that match a glob pattern ------------------------

# Setup: create a folder "output_glob" that contains three CSV files,
# two of which follow the pattern "output_XXX.csv"
dest_folder <- withr::local_tempdir(tmpdir = "output_glob")
dir.create(dest_folder, showWarnings = FALSE)
dest1 <- file.path(dest_folder, "output_1.csv")
dest2 <- file.path(dest_folder, "output_2.csv")
dest3 <- file.path(dest_folder, "other_output.csv")

write.csv(mtcars[1:16, ], dest1, row.names = FALSE)
write.csv(mtcars[17:32, ], dest2, row.names = FALSE)
write.csv(iris, dest3, row.names = FALSE)
list.files(dest_folder)

# Import only the files whose name match "output_XXX.csv" as a LazyFrame
scan_csv_polars(paste0(dest_folder, "/output_*.csv")) |>
  arrange(mpg) |>
  compute()
\dontshow{\}) # examplesIf}
}
