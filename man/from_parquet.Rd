% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/read_scan.R
\name{read_parquet_polars}
\alias{read_parquet_polars}
\alias{scan_parquet_polars}
\title{Import data from parquet file(s)}
\usage{
read_parquet_polars(
  source,
  ...,
  n_rows = NULL,
  row_index_name = NULL,
  row_index_offset = 0L,
  parallel = "auto",
  hive_partitioning = TRUE,
  glob = TRUE,
  rechunk = TRUE,
  low_memory = FALSE,
  storage_options = NULL,
  use_statistics = TRUE,
  cache = TRUE
)

scan_parquet_polars(
  source,
  ...,
  n_rows = NULL,
  row_index_name = NULL,
  row_index_offset = 0L,
  parallel = "auto",
  hive_partitioning = TRUE,
  glob = TRUE,
  rechunk = FALSE,
  low_memory = FALSE,
  storage_options = NULL,
  use_statistics = TRUE,
  cache = TRUE
)
}
\arguments{
\item{source}{Path to a file. You can use globbing with \code{*} to scan/read multiple
files in the same directory (see examples).}

\item{...}{Ignored.}

\item{n_rows}{Maximum number of rows to read.}

\item{row_index_name}{If not \code{NULL}, this will insert a row index column with
the given name into the DataFrame.}

\item{row_index_offset}{Offset to start the row index column (only used if
the name is set).}

\item{parallel}{This determines the direction of parallelism. \code{"auto"} will
try to determine the optimal direction. Can be \code{"auto"}, \code{"columns"},
\code{"row_groups"}, or \code{"none"}.}

\item{hive_partitioning}{Infer statistics and schema from hive partitioned URL
and use them to prune reads.}

\item{glob}{Expand path given via globbing rules.}

\item{rechunk}{In case of reading multiple files via a glob pattern, rechunk
the final DataFrame into contiguous memory chunks.}

\item{low_memory}{Reduce memory usage (will yield a lower performance).}

\item{storage_options}{Experimental. List of options necessary to scan
parquet files from different cloud storage providers (GCP, AWS, Azure).
See the 'Details' section.}

\item{use_statistics}{Use statistics in the parquet file to determine if pages
can be skipped from reading.}

\item{cache}{Cache the result after reading.}
}
\description{
\code{read_parquet_polars()} imports the data as a Polars DataFrame.

\code{scan_parquet_polars()} imports the data as a Polars LazyFrame.
}
\details{
\subsection{Connecting to cloud providers}{

Polars supports scanning parquet files from different cloud providers.
The cloud providers currently supported are AWS, GCP, and Azure.
The supported keys to pass to the \code{storage_options} argument can be found
here:
\itemize{
\item \href{https://docs.rs/object_store/latest/object_store/aws/enum.AmazonS3ConfigKey.html}{aws}
\item \href{https://docs.rs/object_store/latest/object_store/gcp/enum.GoogleConfigKey.html}{gcp}
\item \href{https://docs.rs/object_store/latest/object_store/azure/enum.AzureConfigKey.html}{azure}
}
\subsection{Implementation details}{
\itemize{
\item Currently it is impossible to scan public parquet files from GCP without
a valid service account. Be sure to always include a service account in the
\code{storage_options} argument.
}
}

}
}
