% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/read_scan.R
\name{from_parquet}
\alias{from_parquet}
\alias{read_parquet_polars}
\alias{scan_parquet_polars}
\title{Import data from Parquet file(s)}
\usage{
read_parquet_polars(
  source,
  ...,
  n_rows = NULL,
  row_index_name = NULL,
  row_index_offset = 0L,
  parallel = "auto",
  hive_partitioning = NULL,
  hive_schema = NULL,
  try_parse_hive_dates = TRUE,
  glob = TRUE,
  rechunk = TRUE,
  low_memory = FALSE,
  storage_options = NULL,
  use_statistics = TRUE,
  cache = TRUE,
  include_file_paths = NULL
)

scan_parquet_polars(
  source,
  ...,
  n_rows = NULL,
  row_index_name = NULL,
  row_index_offset = 0L,
  parallel = "auto",
  hive_partitioning = NULL,
  hive_schema = NULL,
  try_parse_hive_dates = TRUE,
  glob = TRUE,
  rechunk = FALSE,
  low_memory = FALSE,
  storage_options = NULL,
  use_statistics = TRUE,
  cache = TRUE,
  include_file_paths = NULL
)
}
\arguments{
\item{source}{Path(s) to a file or directory. When needing to authenticate
for scanning cloud locations, see the \code{storage_options} parameter.}

\item{...}{These dots are for future extensions and must be empty.}

\item{n_rows}{Stop reading from the source after reading \code{n_rows}.}

\item{row_index_name}{If not \code{NULL}, this will insert a row index column with
the given name.}

\item{row_index_offset}{Offset to start the row index column (only used if
the name is set by \code{row_index_name}).}

\item{parallel}{This determines the direction and strategy of parallelism.
\itemize{
\item \code{"auto"} (default): Will try to determine the optimal direction.
\item \code{"prefiltered"}: \ifelse{html}{\href{https://lifecycle.r-lib.org/articles/stages.html#experimental}{\figure{lifecycle-experimental.svg}{options: alt='[Experimental]'}}}{\strong{[Experimental]}}
Strategy first evaluates the pushed-down predicates in parallel and
determines a mask of which rows to read. Then, it parallelizes over both the
columns and the row groups while filtering out rows that do not need to be
read. This can provide significant speedups for large files (i.e. many
row-groups) with a predicate that filters clustered rows or filters heavily.
In other cases, prefiltered may slow down the scan compared other strategies.
Falls back to \code{"auto"} if no predicate is given.
\item \code{"columns"}, \code{"row_groups"}: Use the specified direction.
\item \code{"none"}: No parallelism.
}}

\item{hive_partitioning}{Infer statistics and schema from Hive partitioned
sources and use them to prune reads. If \code{NULL} (default), it is automatically
enabled when a single directory is passed, and otherwise disabled.}

\item{hive_schema}{\ifelse{html}{\href{https://lifecycle.r-lib.org/articles/stages.html#experimental}{\figure{lifecycle-experimental.svg}{options: alt='[Experimental]'}}}{\strong{[Experimental]}}
A list containing the column names and data types of the columns by
which the data is partitioned, e.g. \code{list(a = pl$String, b = pl$Float32)}.
If \code{NULL} (default), the schema of the Hive partitions is inferred.}

\item{try_parse_hive_dates}{Whether to try parsing hive values as date /
datetime types.}

\item{glob}{Expand path given via globbing rules.}

\item{rechunk}{Reallocate to contiguous memory when all chunks/files are parsed.}

\item{low_memory}{Reduce memory pressure at the expense of performance}

\item{storage_options}{Named vector containing options that indicate how to
connect to a cloud provider. The cloud providers currently supported are
AWS, GCP, and Azure.
See supported keys here:
\itemize{
\item \href{https://docs.rs/object_store/latest/object_store/aws/enum.AmazonS3ConfigKey.html}{aws}
\item \href{https://docs.rs/object_store/latest/object_store/gcp/enum.GoogleConfigKey.html}{gcp}
\item \href{https://docs.rs/object_store/latest/object_store/azure/enum.AzureConfigKey.html}{azure}
\item Hugging Face (\verb{hf://}): Accepts an API key under the token parameter
\code{c(token = YOUR_TOKEN)} or by setting the \code{HF_TOKEN} environment
variable.
}

If \code{storage_options} is not provided, Polars will try to infer the
information from environment variables.}

\item{use_statistics}{Use statistics in the parquet to determine if pages
can be skipped from reading.}

\item{cache}{Cache the result after reading.}

\item{include_file_paths}{Include the path of the source file(s) as a column
with this name.}
}
\description{
\code{read_parquet_polars()} imports the data as a Polars DataFrame.

\code{scan_parquet_polars()} imports the data as a Polars LazyFrame.
}
