% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/write.R
\name{write_parquet_polars}
\alias{write_parquet_polars}
\title{Export data to Parquet file(s)}
\usage{
write_parquet_polars(
  .data,
  file,
  ...,
  compression = "zstd",
  compression_level = 3,
  statistics = TRUE,
  row_group_size = NULL,
  data_pagesize_limit = NULL
)
}
\arguments{
\item{.data}{A Polars DataFrame.}

\item{file}{File path to which the result should be written.}

\item{...}{Ignored.}

\item{compression}{The compression method. One of :
\itemize{
\item "uncompressed"
\item "zstd" (default): good compression performance
\item "lz4": fast compression / decompression
\item "snappy": more backwards compatibility guarantees when you deal with older
parquet readers.
\item "gzip", "lzo", "brotli"
}}

\item{compression_level}{The level of compression to use (default is 3). Only
used if \code{compression} is one of "gzip", "brotli", or "zstd". Higher
compression means smaller files on disk.
\itemize{
\item "gzip" : min-level = 0, max-level = 10
\item "brotli" : min-level = 0, max-level = 11
\item "zstd" : min-level = 1, max-level = 22.
}}

\item{statistics}{Whether to compute and write column statistics (default is
\code{FALSE}). This requires more computations.}

\item{row_group_size}{Size of the row groups in number of rows. If \code{NULL}
(default), the chunks of the DataFrame are used. Writing in smaller chunks
may reduce memory pressure and improve writing speeds.}

\item{data_pagesize_limit}{If \code{NULL} (default), the limit will be around 1MB.}
}
\value{
The input DataFrame.
}
\description{
Export data to Parquet file(s)
}
\examples{
\dontshow{if (requireNamespace("nanoparquet")) (if (getRversion() >= "3.4") withAutoprint else force)(\{ # examplesIf}
dest <- tempfile(fileext = ".parquet")
mtcars |>
  as_polars_df() |>
  write_parquet_polars(dest)

nanoparquet::read_parquet(dest)
\dontshow{\}) # examplesIf}
}
