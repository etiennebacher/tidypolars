% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/sink_parquet.R
\name{sink_parquet}
\alias{sink_parquet}
\title{Stream output to a parquet file}
\usage{
sink_parquet(
  .data,
  path,
  compression = "zstd",
  compression_level = 3,
  statistics = FALSE,
  row_group_size = NULL,
  data_pagesize_limit = NULL,
  maintain_order = TRUE,
  type_coercion = TRUE,
  predicate_pushdown = TRUE,
  projection_pushdown = TRUE,
  simplify_expression = TRUE,
  slice_pushdown = TRUE,
  no_optimization = FALSE
)
}
\arguments{
\item{.data}{A Polars LazyFrame.}

\item{path}{Output file (must be a \code{.parquet} file).}

\item{compression}{The compression method. One of :
\itemize{
\item "uncompressed"
\item "zstd" (default): good compression performance
\item “lz4": fast compression/decompression
\item "snappy": more backwards compatibility guarantees when you deal with older
parquet readers.
\item "gzip", "lzo", "brotli"
}}

\item{compression_level}{The level of compression to use (default is 3). Only
used if \code{compression} is one of "gzip", "brotli", or "zstd"⁠. Higher
compression means smaller files on disk.
\itemize{
\item "gzip" : min-level = 0, max-level = 10
\item "brotli" : min-level = 0, max-level = 11
\item "zstd" : min-level = 1, max-level = 22.
}}

\item{statistics}{Whether to compute and write column statistics (default is
\code{FALSE}). This requires more computations.}

\item{row_group_size}{Size of the row groups in number of rows. If \code{NULL}
(default), the chunks of the DataFrame are used. Writing in smaller chunks
may reduce memory pressure and improve writing speeds.}

\item{data_pagesize_limit}{If \code{NULL} (default), the limit will be around 1MB.}

\item{maintain_order}{Whether maintain the order the data was processed
(default is \code{TRUE}). Setting this to \code{FALSE} will be slightly faster.}

\item{type_coercion}{Coerce types such that operations succeed and run on
minimal required memory (default is \code{TRUE}).}

\item{predicate_pushdown}{Applies filters as early as possible at scan level
(default is \code{TRUE}).}

\item{projection_pushdown}{Select only the columns that are needed at the scan
level (default is \code{TRUE}).}

\item{simplify_expression}{Various optimizations, such as constant folding
and replacing expensive operations with faster alternatives (default is
\code{TRUE}).}

\item{slice_pushdown}{Only load the required slice from the scan Don't
materialize sliced outputs level. Don't materialize sliced outputs (default
is \code{TRUE}).}

\item{no_optimization}{Sets the following optimizations to \code{FALSE}:
\code{predicate_pushdown}, \code{projection_pushdown},  \code{slice_pushdown},
\code{simplify_expression}. Default is \code{FALSE}.}
}
\value{
Writes a \code{.parquet} file with the content of the LazyFrame.
}
\description{
This function allows to stream a LazyFrame that is larger than RAM directly
to a \code{.parquet} file without collecting it in the R session, thus preventing
crashes because of too small memory.
}
\examples{
\dontrun{
# This is an example workflow where sink_parquet() is not very useful because
# the data would fit in memory. It simply is an example of using it at the
# end of a piped workflow.

# Create files for the CSV input and the Parquet output:
file_csv <- tempfile(fileext = ".csv")
file_parquet <- tempfile(fileext = ".parquet")

# Write some data in a CSV file
fake_data <- do.call("rbind", rep(list(mtcars), 1000))
write.csv(fake_data, file = file_csv)

# In a new R session, we could read this file as a LazyFrame, do some operations,
# and write it to a parquet file without ever collecting it in the R session:
polars::pl$scan_csv(file_csv) |>
  pl_filter(cyl \%in\% c(4, 6), mpg > 22) |>
  pl_mutate(
    hp_gear_ratio = hp / gear
  ) |>
  sink_parquet(path = file_parquet)

}
}
